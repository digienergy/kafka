from confluent_kafka import Consumer
from dotenv import load_dotenv
import time
import os
import json
from datetime import datetime
from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, Float, DateTime, text, func
from sqlalchemy.inspection import inspect
from modbus_mapping import GoodWe
# import queue
import threading
import concurrent.futures
from ecu_1051 import CleanECU1051
from pathlib import Path
from confluent_kafka.admin import AdminClient, NewTopic
from confluent_kafka import KafkaException
import random
from concurrent.futures import ThreadPoolExecutor

load_dotenv()

KAFKA_BROKER = os.getenv('KAFKA_BROKER')
DATABASE_URL = os.getenv("DATABASE_URL")

# INFLUXDB_HOST = os.getenv("INFLUXDB_HOST", "localhost")
# INFLUXDB_PORT = int(os.getenv("INFLUXDB_PORT", 8086))
# INFLUXDB_DB = os.getenv("INFLUXDB_DB", "iot_metrics")


engine = create_engine(DATABASE_URL)
Session = sessionmaker(bind=engine)

kafka_topic_list = []
postgres_schema_list = []

column_cache = {}

def topic_divide(topic):
    parts = topic.split("/")  

    if len(parts) != 5: 
       return None, None, None, None, None

    customer_id,inverter_brand,inverter_devicetype, datalogger_brand, datalogger_sn  = parts

    return customer_id,inverter_brand,inverter_devicetype, datalogger_brand, datalogger_sn 

def list_kafka_topics():
    """ç²å– Kafka ä¸­æ‰€æœ‰çš„ Topics"""
    global kafka_topic_list
    admin_client = AdminClient({'bootstrap.servers': KAFKA_BROKER})
    metadata = admin_client.list_topics(timeout=10)

    kafka_topic_list = [
        topic for topic in metadata.topics.keys()
        if not topic.startswith("__") and topic.isdigit()
    ]

def list_postgres_schemas():
    """åˆ—å‡º PostgreSQL ä¸­æ‰€æœ‰æ•¸å­—åç¨±çš„ Schemas"""
    global postgres_schema_list
    with engine.connect() as connection:
        schema_query = text("""
            SELECT schema_name FROM information_schema.schemata
            WHERE schema_name ~ '^[0-9]+$'  -- åªé¸æ“‡å…¨æ•¸å­—çš„ schema
              AND schema_name NOT IN ('pg_catalog', 'information_schema', 'pg_toast')
        """)
        postgres_schema_list = [row[0] for row in connection.execute(schema_query)]

def setup_postgres_from_kafka():
    """ä¾æ“š Kafka Topics å»ºç«‹ PostgreSQL Schemas å’Œ Tables"""
    global postgres_schema_list,kafka_topic_list

    print(f"{datetime.now()} setup_postgres_from_kafka kafka_topics:",kafka_topic_list)
    print(f"{datetime.now()} setup_postgres_from_kafka schema_list:",postgres_schema_list)

    for topic in kafka_topic_list or not topic.isdigit():
        if topic.startswith("__"):  # å¿½ç•¥ Kafka å…§å»º Topic
            continue

        # ç¢ºä¿ PostgreSQL Schema å­˜åœ¨
        if topic not in postgres_schema_list:
            create_schema(topic)
            create_table(topic, "inverter")
            create_table(topic, "alarm")

            postgres_schema_list.append(topic)

def check_and_update_schema_tables():
    """æª¢æŸ¥ PostgreSQL schemas æ˜¯å¦åŒ…å« 'inverter' å’Œ 'alarm' è¡¨ï¼Œä¸¦æ›´æ–° column_cache"""
    global column_cache,postgres_schema_list

    with engine.connect() as connection:
        for schema in postgres_schema_list:
            # æŸ¥è©¢è©² schema ä¸‹çš„ tables
            table_query = text(f"""
                SELECT table_name FROM information_schema.tables
                WHERE table_schema = '{schema}'
            """)
            existing_tables = {row[0] for row in connection.execute(table_query)}

            # ç¢ºä¿ `inverter` å’Œ `alarm` è¡¨éƒ½å­˜åœ¨
            required_tables = ["inverter", "alarm"]
            for table in required_tables:
                if table not in existing_tables:
                    create_table(schema, table)  # å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œå‰‡å»ºç«‹

            # åˆå§‹åŒ– schema å±¤ç´š
            if schema not in column_cache:
                column_cache[schema] = {}

            # æŸ¥è©¢ç¾æœ‰çš„æ¬„ä½ä¸¦æ›´æ–° column_cache
            for table in required_tables:
                column_query = text(f"""
                    SELECT column_name FROM information_schema.columns
                    WHERE table_schema = '{schema}' AND table_name = '{table}'
                """)
                existing_columns = {row[0] for row in connection.execute(column_query)}
                
                # æ›´æ–°å¿«å–
                column_cache[schema][table] = existing_columns

    print(f"{datetime.now()} Schema and table validation completed. column_cache updated ")

def check_and_create_topic(topic_name):
    """å¦‚æœ Kafka Topic ä¸å­˜åœ¨ï¼Œå‰‡å»ºç«‹æ–°çš„"""
    admin_client = AdminClient({'bootstrap.servers': KAFKA_BROKER})

    # ç²å–ç›®å‰æ‰€æœ‰ Kafka Topics
    existing_topics = admin_client.list_topics(timeout=10).topics.keys()

    if topic_name not in existing_topics:
        print(f"{datetime.now()} å»ºç«‹æ–°çš„ Kafka Topic: {topic_name}")
        new_topic = NewTopic(topic_name, 
                             num_partitions=1, 
                             replication_factor=1)
        admin_client.create_topics([new_topic])
    else:
        print(f"{datetime.now()} Kafka Topic {topic_name} å·²å­˜åœ¨")


def create_schema(schema_name):
    """æª¢æŸ¥ Schema æ˜¯å¦å­˜åœ¨ï¼Œè‹¥ä¸å­˜åœ¨å‰‡å‰µå»º"""

    global postgres_schema_list

    if schema_name in postgres_schema_list:
        return

    with engine.begin() as connection:

        connection.execute(text(f'CREATE SCHEMA "{schema_name}"'))
        connection.commit()
        postgres_schema_list.append(schema_name)

        print(f"{datetime.now()} Schema '{schema_name}' created.")
        

def create_table(schema_name, table_name):
    """å‰µå»º"""
    metadata = MetaData()
    
    columns = [
        Column("id", Integer, primary_key=True, autoincrement=True),
        Column("timestamp", DateTime, nullable=False,server_default=func.now())
    ]
    
    new_table = Table(table_name, metadata, *columns, schema=schema_name)
    metadata.create_all(engine)  

    if schema_name not in column_cache:
        column_cache[schema_name] = {}

    column_cache[schema_name][table_name] = {"id", "timestamp"}

    print(f"{datetime.now()} Table '{table_name}' created in schema '{schema_name}'.")

def create_columns(schema_name, table_name, data):
    """æª¢æŸ¥ä¸¦æ–°å¢ Kafka è¨Šæ¯ä¸­å‡ºç¾çš„æ–°æ¬„ä½"""
    global column_cache

    if schema_name not in column_cache:
        column_cache[schema_name] = {}
        
    # ä½¿ç”¨é€£æ¥å–å¾—ç•¶å‰æ¬„ä½
    if table_name not in column_cache[schema_name]:
        with engine.connect() as connection:
            existing_columns = {row[0] for row in connection.execute(text(f"""
                SELECT column_name FROM information_schema.columns 
                WHERE table_schema = '{schema_name}' AND table_name = '{table_name}'
            """))}
        column_cache[schema_name][table_name] = existing_columns 
    
    existing_columns = column_cache[schema_name][table_name]
    new_columns = [k for k in data.keys() if k not in existing_columns]

    if not new_columns:
        return  # å¦‚æœæ²’æœ‰æ–°æ¬„ä½ï¼Œç›´æ¥è¿”å›
        
    with engine.connect() as connection:
        for key in new_columns:
            value = data[key]
            if key == "errormessage" or key == "warningcode":  
                column_type = "BIGINT"  # **éŒ¯èª¤ç¢¼å’Œè­¦å‘Šç¢¼ä½¿ç”¨ BIGINT**
            elif key.endswith("time"):
                column_type = "TIMESTAMP WITHOUT TIME ZONE"  # **æ™‚é–“é¡å‹**
            elif key.endswith("voltage") or key.endswith("current") or key.endswith("power") or key.endswith("energy") or key.endswith("frequency"):
                column_type = "FLOAT"  # **é›»å£“ã€é›»æµã€åŠŸç‡ã€é »ç‡ç­‰**
            elif isinstance(value, bool):
                column_type = "BOOLEAN"  # **å¸ƒæ—å€¼**
            elif isinstance(value, int):
                column_type = "INTEGER"  # **æ•´æ•¸**
            elif isinstance(value, float):
                column_type = "DOUBLE PRECISION"  # **æµ®é»æ•¸**
            else:
                column_type = "CHARACTER VARYING(125)"  # **é è¨­ç‚ºå­—ä¸²**

            alter_sql = f'ALTER TABLE "{schema_name}"."{table_name}" ADD COLUMN "{key}" {column_type}'
            connection.execute(text(alter_sql))
            print(f"{datetime.now()} Added new column '{key}' to table '{table_name}' in schema '{schema_name}' with type '{column_type}'.")

        # æ›´æ–°å¿«å–ï¼Œé¿å…ä¸‹æ¬¡é‡è¤‡æŸ¥è©¢
        column_cache[schema_name][table_name].update(new_columns)

        connection.commit()

def create_index(schema_name, table_name):
    """
    ç‚ºæŒ‡å®šè³‡æ–™è¡¨å»ºç«‹è¤‡åˆç´¢å¼•ï¼šserialnumber å’Œ timestamp
    """
    if table_name == "inverter":
        index_name = f"{table_name}_timestamp_serialnumber_idx"
        with engine.connect() as connection:
            # æª¢æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            check_index_query = text(f"""
                SELECT 1 FROM pg_indexes 
                WHERE schemaname = '{schema_name}' AND indexname = '{index_name}'
            """)
            result = connection.execute(check_index_query).fetchone()

            if result:
                print(f"{datetime.now()} Index '{index_name}' already exists.")
            else:
                # å»ºç«‹è¤‡åˆç´¢å¼•
                create_index_query = text(f"""
                    CREATE INDEX "{index_name}" ON "{schema_name}"."{table_name}" ("timestamp", "serialnumber")
                """)
                connection.execute(create_index_query)
                print(f"{datetime.now()} Created index '{index_name}' on '{table_name}' (timestamp,serialnumber).")
    else:
        index_name = f"{table_name}_timestamp_serialnumber_errormessage_idx"
        with engine.connect() as connection:
            # æª¢æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            check_index_query = text(f"""
                SELECT 1 FROM pg_indexes 
                WHERE schemaname = '{schema_name}' AND indexname = '{index_name}'
            """)
            result = connection.execute(check_index_query).fetchone()

            if result:
                print(f"{datetime.now()} Index '{index_name}' already exists.")
            else:
                # å»ºç«‹è¤‡åˆç´¢å¼•
                create_index_query = text(f"""
                    CREATE INDEX "{index_name}" ON "{schema_name}"."{table_name}" ("timestamp", "serialnumber","errormessage")
                """)
                connection.execute(create_index_query)
                print(f"{datetime.now()} Created index '{index_name}' on '{table_name}' (timestamp,serialnumber,errormessage).")

def create_constraints(schema_name, table_name):
    """ç‚ºæŒ‡å®šè³‡æ–™è¡¨æ–°å¢å”¯ä¸€ç´„æŸ (UNIQUE) åœ¨ collecttime æ¬„ä½"""
    with engine.connect() as connection:
        try:
            # å”¯ä¸€ç´„æŸï¼šç¢ºä¿ `collecttime` ä¸é‡è¤‡
            constraint_name = f"{table_name}_collecttime_unique"

            # æª¢æŸ¥ç´„æŸæ˜¯å¦å·²ç¶“å­˜åœ¨ï¼Œé¿å…é‡è¤‡å»ºç«‹
            check_constraint_query = text(f"""
                SELECT conname 
                FROM pg_constraint 
                WHERE conrelid = '{schema_name}.{table_name}'::regclass 
                AND conname = '{constraint_name}'
            """)
            result = connection.execute(check_constraint_query).fetchone()

            if not result:
                connection.execute(text(f"""
                    ALTER TABLE "{schema_name}"."{table_name}" 
                    ADD CONSTRAINT "{constraint_name}" UNIQUE (collecttime)
                """))
                print(f"{datetime.now()} Unique constraint added to '{schema_name}.{table_name}' on collecttime.")
            else:
                print(f"{datetime.now()} Unique constraint already exists on '{schema_name}.{table_name}'.")
        except Exception as e:
            print(f"{datetime.now()} Error adding unique constraint to '{schema_name}.{table_name}': {e}")


def consumer_worker():
    """Kafka æ¶ˆè²»è€…æŒçºŒç›£è½ Kafka topic ä¸¦å°‡æ•¸æ“šå¯«å…¥è³‡æ–™åº«"""
    global kafka_topic_list
    old_kafka_topic_list=[]
    consumer_config = {
        'bootstrap.servers': KAFKA_BROKER,
        'group.id': 'iot_consumer_group',
        'auto.offset.reset': 'earliest',
        'enable.auto.commit': True,  # âœ… è‡ªå‹•æäº¤ offset
        'auto.commit.interval.ms': 5000  # âœ… æ¯ 5 ç§’è‡ªå‹•æäº¤
    }
    consumer = Consumer(consumer_config)

    while not kafka_topic_list:
        print(f"{datetime.now()}  No available Kafka topics at startup. Retrying in 60 seconds...")
        list_kafka_topics()  # é‡æ–°ç²å– Kafka topic åˆ—è¡¨
        time.sleep(60)

    consumer.subscribe(kafka_topic_list)
    old_kafka_topic_list = kafka_topic_list

    last_refresh_time = time.time() 

    while True:
        try:
            if time.time() - last_refresh_time >= 3600:
                print(f"{datetime.now()} Refreshing Kafka topic subscriptions...")
                if len(kafka_topic_list)-len(old_kafka_topic_list) > 0:
                    consumer.subscribe(kafka_topic_list)  # é‡æ–°è¨‚é–±
                    old_kafka_topic_list = kafka_topic_list
                last_refresh_time = time.time()  # æ›´æ–°ä¸Šæ¬¡è¨‚é–±æ™‚é–“

            messages = consumer.poll(timeout=10)
            
            if not messages:
                continue

            if messages is None or messages.error():
                print(f"{datetime.now()} Kafka consumer error: {messages.error()}")
                continue

            data = json.loads(messages.value().decode('utf-8'))

            kafka_topic = messages.topic()

            inverter_brand = data.pop("inverter_brand", None)  # Remove "inverter_brand" from data
            inverter_devicetype = data.pop("devicetype", None)  # Remove "inverter_devicetype"

            #write_to_influx_db(kafka_topic, inverter_brand, inverter_devicetype, data)
            write_to_postgresql_db(kafka_topic, inverter_brand, inverter_devicetype, data)
            error_message = random.choice(list(GoodWe(devicetype="GW50KN-MT").ERROR_MESSAGE_MAP.keys())) #å»ºç«‹éŒ¯èª¤å‡è³‡æ–™
            data["errormessage"] = error_message
            if data["errormessage"] != 0:

                write_to_postgresql_db(kafka_topic, inverter_brand, inverter_devicetype, data)


        except KafkaException as e:
            print(f"{datetime.now()} Kafka error: {e}. Retrying in 5 seconds...")
            time.sleep(5)
        except Exception as e:
            print(f"{datetime.now()} Unexpected error in Kafka consumer: {e}")

# ğŸš€ InfluxDB è³‡æ–™å¯«å…¥
def write_to_influx_db(data):
    pass
    try:
        influx_json = [{
            "measurement": "sensor_data",
            "tags": {
                "device": data["serialnumber"],
                "customer": data["customer_id"]
            },
            "time": data["collecttime"],
            "fields": {k: float(v) for k, v in data.items() if isinstance(v, (int, float))}
        }]
        #influx_client.write_points(influx_json)
        print(f"{datetime.now()} InfluxDB å¯«å…¥æˆåŠŸ: {data}")
    except Exception as e:
        print(f"{datetime.now()} InfluxDB å¯«å…¥å¤±æ•—: {e}")

def write_to_postgresql_db(kafka_topic,inverter_brand, inverter_devicetype, data):
    """Function to write data into PostgreSQL."""
    global postgres_schema_list
    global column_cache 

    schema_name = kafka_topic

    if schema_name not in postgres_schema_list:
        create_schema(schema_name)
        create_table(schema_name, "inverter")
        create_table(schema_name, "alarm")

    missing_columns = set(data.keys()) - column_cache[schema_name]["inverter"]
    
    if missing_columns:
        create_columns(schema_name, "inverter", data)  # å‰µå»ºç¼ºå°‘çš„æ¬„ä½
        create_index(schema_name, "inverter")
        create_constraints(schema_name, "inverter")
        column_cache[schema_name]["inverter"].update(missing_columns)  # æ›´æ–°å¿«å–

    if data["errormessage"] == 0:
        with engine.begin() as connection:
            insert_query = text(f"""
                INSERT INTO "{schema_name}"."{"inverter"}" 
                ({', '.join([f'"{k}"' for k in data.keys()])}) 
                VALUES ({', '.join([f':{k}' for k in data.keys()])})
            """)
            connection.execute(insert_query, data)

            print(f"{datetime.now()} Inserted data into {schema_name}.inverter")
        return 

    if inverter_brand == "goodwe"  and data["errormessage"] != 0:
        data = GoodWe(inverter_devicetype).get_error_message(data)
 
    missing_columns = set(data.keys()) - column_cache[schema_name]["alarm"]
    
    if missing_columns:
        create_columns(schema_name, "alarm", data)  # å‰µå»ºç¼ºå°‘çš„æ¬„ä½
        create_index(schema_name, "alarm")
        create_constraints(schema_name, "alarm")
        column_cache[schema_name]["alarm"].update(missing_columns)  # æ›´æ–°å¿«å–

    # Default case to insert into PostgreSQL
    if data["errormessage"] != 0:
        with engine.begin() as connection:
            insert_query = text(f"""
                INSERT INTO "{schema_name}"."{"alarm"}" 
                ({', '.join([f'"{k}"' for k in data.keys()])}) 
                VALUES ({', '.join([f':{k}' for k in data.keys()])})
            """)
            connection.execute(insert_query, data)

            print(f"{datetime.now()} Inserted data into {schema_name}.alarm")        


def start_kafka_consumer():

    consumer_thread = threading.Thread(target=consumer_worker, daemon=True)
    consumer_thread.start()

if __name__ == "__main__":
    
    try:

        list_kafka_topics()
        list_postgres_schemas()

        setup_postgres_from_kafka()
        check_and_update_schema_tables()

        start_kafka_consumer()

        while True:
            time.sleep(1)

    except Exception as e:
        print(f"{datetime.now()} An error occurred: {e}")
